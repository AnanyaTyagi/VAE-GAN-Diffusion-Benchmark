{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM2vwkCttvngTA4au5dIGvA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnanyaTyagi/VAE-GAN-Diffusion-Benchmark/blob/main/GANImage_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ohtqFMvPcT6",
        "outputId": "a9734833-0c79-4ea3-f4e2-4db10d35c5e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-75e0568e-7b4d-d12b-eb13-7094963b77e5)\n",
            "Mounted at /content/drive\n",
            "✅ Drive mounted. Saving to: /content/drive/MyDrive/gan_cifar10_runs\n",
            "Device: cuda\n",
            "OUT_DIR: /content/drive/MyDrive/gan_cifar10_runs\n"
          ]
        }
      ],
      "source": [
        "# --- GPU + Drive ---\n",
        "!nvidia-smi -L || true\n",
        "\n",
        "from google.colab import drive\n",
        "import os, random, json, math, time\n",
        "import numpy as np\n",
        "drive_ok = True\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    OUT_DIR = \"/content/drive/MyDrive/gan_cifar10_runs\"   # change if you like\n",
        "    print(\"✅ Drive mounted. Saving to:\", OUT_DIR)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Drive mount failed, saving locally. Error:\", e)\n",
        "    OUT_DIR = \"/content/gan_cifar10_runs\"\n",
        "    drive_ok = False\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- Repro/Device ---\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# --- Small helpers ---\n",
        "from torchvision.utils import save_image, make_grid\n",
        "def denorm(x):                      # [-1,1] -> [0,1]\n",
        "    return (x.clamp(-1,1) + 1)/2\n",
        "\n",
        "print(\"Device:\", device)\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# ---------- Generator ----------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=128, fm=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # (N, latent_dim, 1, 1) -> (N, fm*8, 4, 4)\n",
        "            nn.ConvTranspose2d(latent_dim, fm*8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(fm*8), nn.ReLU(True),\n",
        "\n",
        "            # 4->8\n",
        "            nn.ConvTranspose2d(fm*8, fm*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(fm*4), nn.ReLU(True),\n",
        "\n",
        "            # 8->16\n",
        "            nn.ConvTranspose2d(fm*4, fm*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(fm*2), nn.ReLU(True),\n",
        "\n",
        "            # 16->32\n",
        "            nn.ConvTranspose2d(fm*2, fm, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(fm), nn.ReLU(True),\n",
        "\n",
        "            # (N, fm, 32, 32) -> (N, 3, 32, 32)\n",
        "            nn.ConvTranspose2d(fm, 3, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),  # [-1,1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "# ---------- Discriminator ----------\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, fm=64):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, fm, 3, 1, 1, bias=False),   # 32x32\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(fm, fm*2, 4, 2, 1, bias=False),  # 16x16\n",
        "            nn.BatchNorm2d(fm*2), nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(fm*2, fm*4, 4, 2, 1, bias=False), # 8x8\n",
        "            nn.BatchNorm2d(fm*4), nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(fm*4, fm*8, 4, 2, 1, bias=False), # 4x4\n",
        "            nn.BatchNorm2d(fm*8), nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.head = nn.Conv2d(fm*8, 1, 4, 1, 0, bias=False)  # NO sigmoid\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.features(x)\n",
        "        out = self.head(h)        # shape (N,1,1,1)\n",
        "        return out.view(-1)       # shape (N,)\n",
        "\n",
        "\n",
        "# ---------- Weights init (DCGAN) ----------\n",
        "def weights_init(m):\n",
        "    name = m.__class__.__name__\n",
        "    if name.find('Conv') != -1 or name.find('ConvTranspose') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif name.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "# ---------- Data ----------\n",
        "BATCH_SIZE = 128\n",
        "tfm = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),  # [-1,1]\n",
        "])\n",
        "trainset = datasets.CIFAR10(\"./data\", train=True, download=True, transform=tfm)\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"Train batches:\", len(trainloader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT8dxmBBPh2L",
        "outputId": "e1d577e7-a79f-43aa-954d-b7d1fa9b7fcd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:15<00:00, 10.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "# --- Hyperparams ---\n",
        "LATENT_DIM = 128\n",
        "GEN_FM = 128\n",
        "DISC_FM = 64\n",
        "EPOCHS = 100\n",
        "LR_G = 2e-4\n",
        "LR_D = 2e-4\n",
        "BETA1 = 0.5; BETA2 = 0.999\n",
        "\n",
        "# --- Models/Opt/AMP ---\n",
        "G = Generator(LATENT_DIM, GEN_FM).to(device)\n",
        "D = Discriminator(DISC_FM).to(device)\n",
        "G.apply(weights_init); D.apply(weights_init)\n",
        "\n",
        "optG = torch.optim.Adam(G.parameters(), lr=LR_G, betas=(BETA1, BETA2))\n",
        "optD = torch.optim.Adam(D.parameters(), lr=LR_D, betas=(BETA1, BETA2))\n",
        "\n",
        "# ✅ use logits-safe loss\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# ✅ new AMP API (no FutureWarning)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
        "\n",
        "fixed_noise = torch.randn(64, LATENT_DIM, 1, 1, device=device)\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    G.train(); D.train()\n",
        "    pbar = tqdm.tqdm(trainloader, desc=f\"Epoch {epoch}\", unit=\"batch\")\n",
        "    for real, _ in pbar:\n",
        "        real = real.to(device)\n",
        "        N = real.size(0)\n",
        "        real_label = torch.ones(N, device=device)\n",
        "        fake_label = torch.zeros(N, device=device)\n",
        "\n",
        "        # ----- Train D -----\n",
        "        optD.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
        "\n",
        "            out_real = D(real)                        # logits\n",
        "            loss_real = criterion(out_real, real_label)\n",
        "\n",
        "            z = torch.randn(N, LATENT_DIM, 1, 1, device=device)\n",
        "            fake = G(z).detach()\n",
        "            out_fake = D(fake)                        # logits\n",
        "            loss_fake = criterion(out_fake, fake_label)\n",
        "\n",
        "            loss_D = loss_real + loss_fake\n",
        "\n",
        "        scaler.scale(loss_D).backward()\n",
        "        scaler.step(optD)\n",
        "\n",
        "        # ----- Train G -----\n",
        "        optG.zero_grad(set_to_none=True)\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device==\"cuda\")):\n",
        "            z = torch.randn(N, LATENT_DIM, 1, 1, device=device)\n",
        "            fake = G(z)\n",
        "            out = D(fake)                             # logits\n",
        "            loss_G = criterion(out, real_label)       # wants \"real\"\n",
        "\n",
        "        scaler.scale(loss_G).backward()\n",
        "        scaler.step(optG)\n",
        "        scaler.update()\n",
        "\n",
        "        pbar.set_postfix(loss_D=float(loss_D), loss_G=float(loss_G))\n",
        "\n",
        "    # ----- Save epoch artifacts -----\n",
        "    G.eval(); D.eval()\n",
        "    with torch.no_grad():\n",
        "        fake_fixed = G(fixed_noise)\n",
        "    grid_real = make_grid(denorm(real[:64].cpu()), nrow=8)\n",
        "    grid_fake = make_grid(denorm(fake_fixed.cpu()), nrow=8)\n",
        "    combo = torch.cat([grid_real, grid_fake], dim=1)\n",
        "    save_image(combo, os.path.join(OUT_DIR, f\"real_vs_fake_epoch_{epoch:03d}.png\"))\n",
        "\n",
        "    ckpt = {\n",
        "        \"epoch\": epoch,\n",
        "        \"G\": G.state_dict(),\n",
        "        \"D\": D.state_dict(),\n",
        "        \"LATENT_DIM\": LATENT_DIM,\n",
        "        \"GEN_FM\": GEN_FM,\n",
        "        \"DISC_FM\": DISC_FM,\n",
        "    }\n",
        "    torch.save(ckpt, os.path.join(OUT_DIR, f\"dcgan_epoch_{epoch:03d}.pth\"))\n",
        "\n",
        "print(\"Training complete. Files in:\", OUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93Ylc7MjP4ft",
        "outputId": "73cb1d03-acda-432d-82f3-1d5fd7d484b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2072044049.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
            "Epoch 1:   0%|          | 0/391 [00:00<?, ?batch/s]/tmp/ipython-input-2072044049.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
            "/tmp/ipython-input-2072044049.py:66: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
            "  pbar.set_postfix(loss_D=float(loss_D), loss_G=float(loss_G))\n",
            "Epoch 1: 100%|██████████| 391/391 [00:13<00:00, 29.75batch/s, loss_D=0.512, loss_G=7.64]\n",
            "Epoch 2: 100%|██████████| 391/391 [00:06<00:00, 61.12batch/s, loss_D=0.882, loss_G=2.95]\n",
            "Epoch 3: 100%|██████████| 391/391 [00:06<00:00, 60.63batch/s, loss_D=0.336, loss_G=3.91]\n",
            "Epoch 4: 100%|██████████| 391/391 [00:06<00:00, 60.63batch/s, loss_D=0.458, loss_G=3.93]\n",
            "Epoch 5: 100%|██████████| 391/391 [00:06<00:00, 60.67batch/s, loss_D=0.513, loss_G=1.74]\n",
            "Epoch 6: 100%|██████████| 391/391 [00:06<00:00, 58.53batch/s, loss_D=0.562, loss_G=2.67]\n",
            "Epoch 7: 100%|██████████| 391/391 [00:06<00:00, 60.40batch/s, loss_D=0.958, loss_G=3.53]\n",
            "Epoch 8: 100%|██████████| 391/391 [00:06<00:00, 60.59batch/s, loss_D=1.46, loss_G=2.36]\n",
            "Epoch 9: 100%|██████████| 391/391 [00:06<00:00, 59.94batch/s, loss_D=0.828, loss_G=2.63]\n",
            "Epoch 10: 100%|██████████| 391/391 [00:06<00:00, 61.22batch/s, loss_D=0.638, loss_G=1.88]\n",
            "Epoch 11: 100%|██████████| 391/391 [00:06<00:00, 57.30batch/s, loss_D=0.561, loss_G=2.48]\n",
            "Epoch 12: 100%|██████████| 391/391 [00:06<00:00, 61.90batch/s, loss_D=1.29, loss_G=5.1]\n",
            "Epoch 13: 100%|██████████| 391/391 [00:06<00:00, 62.27batch/s, loss_D=0.433, loss_G=2.93]\n",
            "Epoch 14: 100%|██████████| 391/391 [00:06<00:00, 61.75batch/s, loss_D=0.46, loss_G=3.78]\n",
            "Epoch 15: 100%|██████████| 391/391 [00:06<00:00, 60.72batch/s, loss_D=0.557, loss_G=1.81]\n",
            "Epoch 16: 100%|██████████| 391/391 [00:06<00:00, 61.29batch/s, loss_D=0.341, loss_G=3.35]\n",
            "Epoch 17: 100%|██████████| 391/391 [00:06<00:00, 57.39batch/s, loss_D=0.471, loss_G=3.49]\n",
            "Epoch 18: 100%|██████████| 391/391 [00:06<00:00, 60.71batch/s, loss_D=0.406, loss_G=1.98]\n",
            "Epoch 19: 100%|██████████| 391/391 [00:06<00:00, 59.10batch/s, loss_D=0.667, loss_G=3.46]\n",
            "Epoch 20: 100%|██████████| 391/391 [00:06<00:00, 60.03batch/s, loss_D=0.3, loss_G=3.12]\n",
            "Epoch 21: 100%|██████████| 391/391 [00:06<00:00, 61.20batch/s, loss_D=0.36, loss_G=2.6]\n",
            "Epoch 22: 100%|██████████| 391/391 [00:06<00:00, 60.19batch/s, loss_D=0.453, loss_G=2.84]\n",
            "Epoch 23: 100%|██████████| 391/391 [00:06<00:00, 58.79batch/s, loss_D=0.509, loss_G=2.83]\n",
            "Epoch 24: 100%|██████████| 391/391 [00:06<00:00, 59.88batch/s, loss_D=0.394, loss_G=2.36]\n",
            "Epoch 25: 100%|██████████| 391/391 [00:06<00:00, 59.69batch/s, loss_D=0.485, loss_G=2.68]\n",
            "Epoch 26: 100%|██████████| 391/391 [00:06<00:00, 59.61batch/s, loss_D=0.325, loss_G=2.61]\n",
            "Epoch 27: 100%|██████████| 391/391 [00:06<00:00, 61.52batch/s, loss_D=0.197, loss_G=3.94]\n",
            "Epoch 28: 100%|██████████| 391/391 [00:06<00:00, 60.93batch/s, loss_D=0.197, loss_G=2.96]\n",
            "Epoch 29: 100%|██████████| 391/391 [00:06<00:00, 58.65batch/s, loss_D=0.218, loss_G=4.52]\n",
            "Epoch 30: 100%|██████████| 391/391 [00:06<00:00, 61.45batch/s, loss_D=0.154, loss_G=4.2]\n",
            "Epoch 31: 100%|██████████| 391/391 [00:06<00:00, 58.28batch/s, loss_D=0.794, loss_G=5.76]\n",
            "Epoch 32: 100%|██████████| 391/391 [00:06<00:00, 61.33batch/s, loss_D=0.184, loss_G=2.9]\n",
            "Epoch 33: 100%|██████████| 391/391 [00:06<00:00, 60.83batch/s, loss_D=0.406, loss_G=1.08]\n",
            "Epoch 34: 100%|██████████| 391/391 [00:06<00:00, 60.22batch/s, loss_D=0.213, loss_G=2.66]\n",
            "Epoch 35: 100%|██████████| 391/391 [00:06<00:00, 61.63batch/s, loss_D=0.392, loss_G=3.1]\n",
            "Epoch 36: 100%|██████████| 391/391 [00:06<00:00, 60.84batch/s, loss_D=0.0969, loss_G=4]\n",
            "Epoch 37: 100%|██████████| 391/391 [00:06<00:00, 59.37batch/s, loss_D=0.108, loss_G=3.7]\n",
            "Epoch 38: 100%|██████████| 391/391 [00:06<00:00, 61.35batch/s, loss_D=0.204, loss_G=2.73]\n",
            "Epoch 39: 100%|██████████| 391/391 [00:06<00:00, 59.07batch/s, loss_D=0.283, loss_G=3.69]\n",
            "Epoch 40: 100%|██████████| 391/391 [00:06<00:00, 61.84batch/s, loss_D=0.0862, loss_G=3.83]\n",
            "Epoch 41: 100%|██████████| 391/391 [00:06<00:00, 62.01batch/s, loss_D=0.997, loss_G=1.72]\n",
            "Epoch 42: 100%|██████████| 391/391 [00:06<00:00, 61.06batch/s, loss_D=1.01, loss_G=4]\n",
            "Epoch 43: 100%|██████████| 391/391 [00:06<00:00, 62.21batch/s, loss_D=0.158, loss_G=3.12]\n",
            "Epoch 44: 100%|██████████| 391/391 [00:06<00:00, 61.18batch/s, loss_D=0.283, loss_G=3.27]\n",
            "Epoch 45: 100%|██████████| 391/391 [00:06<00:00, 60.48batch/s, loss_D=0.478, loss_G=2.67]\n",
            "Epoch 46: 100%|██████████| 391/391 [00:06<00:00, 59.79batch/s, loss_D=0.326, loss_G=2.9]\n",
            "Epoch 47: 100%|██████████| 391/391 [00:06<00:00, 60.33batch/s, loss_D=0.106, loss_G=3.72]\n",
            "Epoch 48: 100%|██████████| 391/391 [00:06<00:00, 61.02batch/s, loss_D=0.211, loss_G=4.2]\n",
            "Epoch 49: 100%|██████████| 391/391 [00:06<00:00, 61.67batch/s, loss_D=0.113, loss_G=4.18]\n",
            "Epoch 50: 100%|██████████| 391/391 [00:06<00:00, 59.12batch/s, loss_D=0.139, loss_G=4.25]\n",
            "Epoch 51: 100%|██████████| 391/391 [00:06<00:00, 60.04batch/s, loss_D=0.276, loss_G=1.89]\n",
            "Epoch 52: 100%|██████████| 391/391 [00:06<00:00, 59.36batch/s, loss_D=0.21, loss_G=4]\n",
            "Epoch 53: 100%|██████████| 391/391 [00:06<00:00, 59.92batch/s, loss_D=0.111, loss_G=4.83]\n",
            "Epoch 54: 100%|██████████| 391/391 [00:06<00:00, 61.91batch/s, loss_D=0.242, loss_G=1.51]\n",
            "Epoch 55: 100%|██████████| 391/391 [00:06<00:00, 61.26batch/s, loss_D=0.0695, loss_G=3.93]\n",
            "Epoch 56: 100%|██████████| 391/391 [00:06<00:00, 60.60batch/s, loss_D=0.111, loss_G=4.88]\n",
            "Epoch 57: 100%|██████████| 391/391 [00:06<00:00, 59.58batch/s, loss_D=0.586, loss_G=4.96]\n",
            "Epoch 58: 100%|██████████| 391/391 [00:06<00:00, 61.07batch/s, loss_D=1.23, loss_G=0.14]\n",
            "Epoch 59: 100%|██████████| 391/391 [00:06<00:00, 58.65batch/s, loss_D=0.0783, loss_G=3.28]\n",
            "Epoch 60: 100%|██████████| 391/391 [00:06<00:00, 61.35batch/s, loss_D=0.262, loss_G=3.71]\n",
            "Epoch 61: 100%|██████████| 391/391 [00:06<00:00, 62.15batch/s, loss_D=0.141, loss_G=1.99]\n",
            "Epoch 62: 100%|██████████| 391/391 [00:06<00:00, 60.93batch/s, loss_D=0.0686, loss_G=4.43]\n",
            "Epoch 63: 100%|██████████| 391/391 [00:06<00:00, 60.26batch/s, loss_D=0.089, loss_G=2.95]\n",
            "Epoch 64: 100%|██████████| 391/391 [00:06<00:00, 59.66batch/s, loss_D=0.417, loss_G=1.21]\n",
            "Epoch 65: 100%|██████████| 391/391 [00:06<00:00, 57.31batch/s, loss_D=0.087, loss_G=3.87]\n",
            "Epoch 66: 100%|██████████| 391/391 [00:06<00:00, 60.24batch/s, loss_D=0.221, loss_G=5.22]\n",
            "Epoch 67: 100%|██████████| 391/391 [00:06<00:00, 60.59batch/s, loss_D=0.0768, loss_G=4.21]\n",
            "Epoch 68: 100%|██████████| 391/391 [00:06<00:00, 62.09batch/s, loss_D=0.154, loss_G=3.92]\n",
            "Epoch 69: 100%|██████████| 391/391 [00:06<00:00, 62.42batch/s, loss_D=0.0694, loss_G=4.96]\n",
            "Epoch 70: 100%|██████████| 391/391 [00:06<00:00, 60.19batch/s, loss_D=0.0541, loss_G=4.97]\n",
            "Epoch 71: 100%|██████████| 391/391 [00:06<00:00, 60.57batch/s, loss_D=0.0318, loss_G=5.74]\n",
            "Epoch 72: 100%|██████████| 391/391 [00:06<00:00, 60.45batch/s, loss_D=0.527, loss_G=4.65]\n",
            "Epoch 73: 100%|██████████| 391/391 [00:06<00:00, 60.99batch/s, loss_D=0.183, loss_G=3.61]\n",
            "Epoch 74: 100%|██████████| 391/391 [00:06<00:00, 60.58batch/s, loss_D=0.0599, loss_G=4.15]\n",
            "Epoch 75: 100%|██████████| 391/391 [00:06<00:00, 61.29batch/s, loss_D=0.104, loss_G=3.39]\n",
            "Epoch 76: 100%|██████████| 391/391 [00:06<00:00, 61.00batch/s, loss_D=1.96, loss_G=0.253]\n",
            "Epoch 77: 100%|██████████| 391/391 [00:06<00:00, 60.51batch/s, loss_D=0.0344, loss_G=3.36]\n",
            "Epoch 78: 100%|██████████| 391/391 [00:06<00:00, 62.00batch/s, loss_D=1.26, loss_G=0.53]\n",
            "Epoch 79: 100%|██████████| 391/391 [00:06<00:00, 59.77batch/s, loss_D=0.029, loss_G=4.26]\n",
            "Epoch 80: 100%|██████████| 391/391 [00:06<00:00, 60.31batch/s, loss_D=2.39, loss_G=5.38]\n",
            "Epoch 81: 100%|██████████| 391/391 [00:06<00:00, 59.42batch/s, loss_D=0.356, loss_G=3.18]\n",
            "Epoch 82: 100%|██████████| 391/391 [00:06<00:00, 62.60batch/s, loss_D=1.45, loss_G=1.09]\n",
            "Epoch 83: 100%|██████████| 391/391 [00:06<00:00, 60.39batch/s, loss_D=0.0585, loss_G=4.56]\n",
            "Epoch 84: 100%|██████████| 391/391 [00:06<00:00, 61.36batch/s, loss_D=0.0197, loss_G=6.82]\n",
            "Epoch 85: 100%|██████████| 391/391 [00:06<00:00, 61.32batch/s, loss_D=0.0754, loss_G=4.16]\n",
            "Epoch 86: 100%|██████████| 391/391 [00:06<00:00, 59.33batch/s, loss_D=0.0896, loss_G=4.65]\n",
            "Epoch 87: 100%|██████████| 391/391 [00:06<00:00, 59.94batch/s, loss_D=0.0169, loss_G=5.42]\n",
            "Epoch 88: 100%|██████████| 391/391 [00:06<00:00, 60.12batch/s, loss_D=0.459, loss_G=2.43]\n",
            "Epoch 89: 100%|██████████| 391/391 [00:06<00:00, 59.81batch/s, loss_D=0.131, loss_G=3.29]\n",
            "Epoch 90: 100%|██████████| 391/391 [00:06<00:00, 57.72batch/s, loss_D=0.638, loss_G=2.54]\n",
            "Epoch 91: 100%|██████████| 391/391 [00:06<00:00, 60.37batch/s, loss_D=0.0332, loss_G=5.31]\n",
            "Epoch 92: 100%|██████████| 391/391 [00:06<00:00, 61.95batch/s, loss_D=0.0199, loss_G=5.51]\n",
            "Epoch 93: 100%|██████████| 391/391 [00:06<00:00, 62.35batch/s, loss_D=1.17, loss_G=0.841]\n",
            "Epoch 94: 100%|██████████| 391/391 [00:06<00:00, 61.80batch/s, loss_D=0.07, loss_G=4.57]\n",
            "Epoch 95: 100%|██████████| 391/391 [00:06<00:00, 61.85batch/s, loss_D=0.0418, loss_G=5.01]\n",
            "Epoch 96: 100%|██████████| 391/391 [00:06<00:00, 60.96batch/s, loss_D=0.0324, loss_G=4.4]\n",
            "Epoch 97: 100%|██████████| 391/391 [00:06<00:00, 60.74batch/s, loss_D=0.163, loss_G=4.77]\n",
            "Epoch 98: 100%|██████████| 391/391 [00:06<00:00, 59.06batch/s, loss_D=0.0661, loss_G=5.68]\n",
            "Epoch 99: 100%|██████████| 391/391 [00:06<00:00, 59.17batch/s, loss_D=1.8, loss_G=3.23]\n",
            "Epoch 100: 100%|██████████| 391/391 [00:06<00:00, 61.52batch/s, loss_D=0.111, loss_G=4.92]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete. Files in: /content/drive/MyDrive/gan_cifar10_runs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "EXPORT_DIR = Path(OUT_DIR) / \"samples_10k\"\n",
        "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "G.eval()\n",
        "total = 10_000\n",
        "bs = 256\n",
        "saved = 0\n",
        "with torch.no_grad():\n",
        "    while saved < total:\n",
        "        cur = min(bs, total - saved)\n",
        "        z = torch.randn(cur, LATENT_DIM, 1, 1, device=device)\n",
        "        imgs = G(z).cpu()\n",
        "        imgs = denorm(imgs)  # [0,1] for PNG\n",
        "        for i in range(cur):\n",
        "            save_image(imgs[i], EXPORT_DIR / f\"{saved+i:05d}.png\")\n",
        "        saved += cur\n",
        "print(f\"Saved {saved} images to {EXPORT_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhPiwxpwP7FK",
        "outputId": "bd7b872f-28a8-4886-c95f-0e8a4f8ec896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 10000 images to /content/drive/MyDrive/gan_cifar10_runs/samples_10k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"torch-fidelity>=0.3.0\"\n",
        "\n",
        "import os, torch\n",
        "from torch_fidelity import calculate_metrics\n",
        "\n",
        "EXPORT_DIR = str(EXPORT_DIR)  # folder with your 10k GAN samples\n",
        "print(\"Using generated samples from:\", EXPORT_DIR)\n",
        "\n",
        "# --- Temporarily patch torch.load so torch-fidelity can read its cached stats safely ---\n",
        "orig_torch_load = torch.load\n",
        "\n",
        "def _compat_load(*args, **kwargs):\n",
        "    kwargs.setdefault(\"weights_only\", False)  # allow full unpickling for torch-fidelity cache\n",
        "    return orig_torch_load(*args, **kwargs)\n",
        "\n",
        "torch.load = _compat_load\n",
        "\n",
        "try:\n",
        "    # === 1) FID + KID vs CIFAR-10 train ===\n",
        "    metrics_fid = calculate_metrics(\n",
        "        input1=EXPORT_DIR,\n",
        "        input2=\"cifar10-train\",\n",
        "        fid=True,\n",
        "        kid=True,\n",
        "        isc=False,             # ✅ use 'isc', not 'inception_score'\n",
        "        cuda=(device == \"cuda\"),\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    # === 2) Inception Score ONLY on generated images ===\n",
        "    metrics_is = calculate_metrics(\n",
        "        input1=EXPORT_DIR,\n",
        "        fid=False,\n",
        "        kid=False,\n",
        "        isc=True,              # ✅ enable IS here\n",
        "        cuda=(device == \"cuda\"),\n",
        "        verbose=False,\n",
        "    )\n",
        "finally:\n",
        "    # restore original torch.load no matter what\n",
        "    torch.load = orig_torch_load\n",
        "\n",
        "# --- Extract metrics safely ---\n",
        "fid  = metrics_fid.get(\"frechet_inception_distance\", None)\n",
        "kidm = metrics_fid.get(\"kernel_inception_distance_mean\", metrics_fid.get(\"kid_mean\", None))\n",
        "kids = metrics_fid.get(\"kernel_inception_distance_std\",  metrics_fid.get(\"kid_std\",  None))\n",
        "\n",
        "# IS field names can vary slightly by version\n",
        "is_m = (metrics_is.get(\"inception_score_mean\")\n",
        "        or metrics_is.get(\"isc_mean\")\n",
        "        or metrics_is.get(\"inception_score\"))\n",
        "is_s = metrics_is.get(\"inception_score_std\", metrics_is.get(\"isc_std\", None))\n",
        "\n",
        "print(\"\\n=== GAN Metrics ===\")\n",
        "print(f\"FID : {fid:.2f}\" if fid is not None else \"FID : None\")\n",
        "\n",
        "if is_m is None:\n",
        "    print(\"IS  : Not returned\")\n",
        "else:\n",
        "    if is_s is None:\n",
        "        print(f\"IS  : {is_m:.2f}\")\n",
        "    else:\n",
        "        print(f\"IS  : {is_m:.2f} ± {is_s:.2f}\")\n",
        "\n",
        "if kidm is None:\n",
        "    print(\"KID : Not returned\")\n",
        "else:\n",
        "    if kids is None:\n",
        "        print(f\"KID : {kidm:.6f}\")\n",
        "    else:\n",
        "        print(f\"KID : {kidm:.6f} ± {kids:.6f}\")\n",
        "\n",
        "# Save to Drive\n",
        "with open(os.path.join(OUT_DIR, \"metrics.txt\"), \"w\") as f:\n",
        "    f.write(f\"FID: {fid}\\n\")\n",
        "    f.write(f\"IS_mean: {is_m}\\n\")\n",
        "    f.write(f\"IS_std: {is_s}\\n\")\n",
        "    f.write(f\"KID_mean: {kidm}\\n\")\n",
        "    f.write(f\"KID_std: {kids}\\n\")\n",
        "\n",
        "print(\"\\nSaved metrics to:\", os.path.join(OUT_DIR, \"metrics.txt\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfnKXVFqP-Sf",
        "outputId": "027926f7-44ed-4d7f-d2f3-a865d36e8cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using generated samples from: /content/drive/MyDrive/gan_cifar10_runs/samples_10k\n",
            "\n",
            "=== GAN Metrics ===\n",
            "FID : 29.29\n",
            "IS  : 7.04 ± 0.17\n",
            "KID : 0.022578 ± 0.001404\n",
            "\n",
            "Saved metrics to: /content/drive/MyDrive/gan_cifar10_runs/metrics.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a saved checkpoint from Drive and generate a grid of images\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "CKPT_PATH = os.path.join(OUT_DIR, \"dcgan_epoch_020.pth\")  # pick any epoch you saved\n",
        "\n",
        "# Rebuild the same model classes, then load weights\n",
        "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "LATENT_DIM = ckpt.get(\"LATENT_DIM\", 128)\n",
        "GEN_FM     = ckpt.get(\"GEN_FM\", 128)\n",
        "\n",
        "G_loaded = Generator(LATENT_DIM, GEN_FM).to(device)\n",
        "G_loaded.load_state_dict(ckpt[\"G\"])\n",
        "G_loaded.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(64, LATENT_DIM, 1, 1, device=device)\n",
        "    imgs = denorm(G_loaded(z)).cpu()\n",
        "save_image(make_grid(imgs, nrow=8), os.path.join(OUT_DIR, \"generated_grid_from_ckpt.png\"))\n",
        "print(\"Saved:\", os.path.join(OUT_DIR, \"generated_grid_from_ckpt.png\"))\n"
      ],
      "metadata": {
        "id": "PUv3O_DgQAEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1tmH3gwwQDCH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}